{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following trends on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Abstract](#Abstract)\n",
    "\n",
    "[2. Exploratory data analysis](#Exploratory-data-analysis)\n",
    "\n",
    "[3. Topic Modeling using LDA](#A-very-simple-Topic-Modeling-using-LDA)\n",
    "\n",
    "[4. Example of the pipeline](#Example-of-the-pipeline-that-we-will-follow-for-the-LDA-algorithm)\n",
    "\n",
    "[5. Milestone 3](#Milestone-3:-the-data-story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realiazed that it will be hard to achieve the goal stated in milestone 1(detecting fake news). The problem is that we couldn't find a way to define fake news. And also the twitter dataset is not what we expected it to be. For example it doesn't contain the number of times a tweet has been retweeted, the geographical location, number of likes ... So we decided to go in a different, more feasable direction, which is following the process of creating and spreading trends on Twitter. Trying to find patterns between trends and users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.sql import functions as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each data entry has 5 fields:\n",
    "     - language: language of the user \n",
    "     - id: id of the user\n",
    "     - date: date when the tweet was published\n",
    "     - username: username of the user\n",
    "     - content: the tweet\n",
    "     \n",
    "Given that for the moment we consider only rows that have all 5 fields we don't have to deal with missing values.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/datasets/tweets-leon MapPartitionsRDD[21] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "data = sc.textFile(\"/datasets/tweets-leon\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1 We will first clean the data and select only a subset that is useful for this project:\n",
    "    - keep only the tweets that have all 5 fields\n",
    "    - remove urls from the content\n",
    "    - remove emojis\n",
    "    - remove punctuation \n",
    "    - remove stopwords\n",
    "    - apply lemmatization \n",
    "    - keep only english, spanish and french tweets\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o269.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/datasets/tweets-leon\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:64)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5c11fb85c8f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfrist_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfrist_tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sandra/Desktop/spark-1.6.3-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[0;32m-> 1315\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sandra/Desktop/spark-1.6.3-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \"\"\"\n\u001b[1;32m   1266\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sandra/Desktop/spark-1.6.3-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sandra/Desktop/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sandra/Desktop/spark-1.6.3-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sandra/Desktop/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o269.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/datasets/tweets-leon\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:64)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "frist_tweet = data.first()\n",
    "frist_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Chose tweets that have exactly 5 components like normal \n",
    "         (language, id, date, username, content)\"\"\"\n",
    "\n",
    "def selection_tweet(tweet):\n",
    "    return len(tweet.split(\"\\t\")) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"encode tweet\"\n",
    "\n",
    "def encode_tweet(tweet):\n",
    "        return [t.encode(\"utf8\") for t in tweet.split(\"\\t\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en',\n",
       " '345963923251539968',\n",
       " 'Sat Jun 15 18:00:01 +0000 2013',\n",
       " 'Letataleta',\n",
       " 'RT @silsilfani: the world is not a wish-granting machine. dont be surprised when everything always end up disappointing.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_tweet(frist_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on using the filter function as done below we will select only a useful subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"remove urls\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"remove emojis\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"remove punctuation\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lemmatization\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"select a subset of the data\"\"\"\n",
    "\n",
    "data = data.filter(selection_tweet)\n",
    "\n",
    "en_data = data.filter(lambda x : x[:2]=='en')\n",
    "es_data = data.filter(lambda x : x[:2]=='es')\n",
    "fr_data = data.filter(lambda x : x[:2]=='fr')\n",
    "\n",
    "data_2012 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2012')\n",
    "data_2013 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2013')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'fr\\t345963923255730176\\tSat Jun 15 18:00:01 +0000 2013\\t_irem61_\\tRT @DHC_Music: Terrorism ... #FreePalestina http://t.co/OLWnVlW682',\n",
       " u'fr\\t345963923335413761\\tSat Jun 15 18:00:01 +0000 2013\\tHairCutGroup\\tHair by Unihair-boutique\\\\nhttp://t.co/fRGYMrRcQe http://t.co/7sXXzn1x34',\n",
       " u'fr\\t345963923465441280\\tSat Jun 15 18:00:01 +0000 2013\\tChajopico\\tRT @KimberleyNoe: \" J\\'ai rencontr\\xe9 un autre. On a pass\\xe9 des moments formidables. C\\'\\xe9tait bien mais ce n\\'\\xe9tait pas toi. \"',\n",
       " u'fr\\t345963923360604161\\tSat Jun 15 18:00:01 +0000 2013\\tyousinceforever\\tDu coup sa ma blaser de tourner en rond la!',\n",
       " u\"fr\\t345963923398328320\\tSat Jun 15 18:00:01 +0000 2013\\tGraciaDiamond_\\tLa meilleure de #SS7 c'est Ana\\xefs sans aucun doute!!! ;-)\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_fr_tweets = fr_data.take(5)\n",
    "some_fr_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_fr_tweets = [encode_tweet(tweet) for tweet in some_fr_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some french tweets:\n",
      "1 )User name: _irem61_\n",
      "         Tweets: RT @DHC_Music: Terrorism ... #FreePalestina http://t.co/OLWnVlW682\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "2 )User name: HairCutGroup\n",
      "         Tweets: Hair by Unihair-boutique\\nhttp://t.co/fRGYMrRcQe http://t.co/7sXXzn1x34\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "3 )User name: Chajopico\n",
      "         Tweets: RT @KimberleyNoe: \" J'ai rencontré un autre. On a passé des moments formidables. C'était bien mais ce n'était pas toi. \"\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "4 )User name: yousinceforever\n",
      "         Tweets: Du coup sa ma blaser de tourner en rond la!\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "5 )User name: GraciaDiamond_\n",
      "         Tweets: La meilleure de #SS7 c'est Anaïs sans aucun doute!!! ;-)\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Some french tweets:'\n",
    "for ind, t in enumerate(some_fr_tweets):\n",
    "    print ind + 1,')User name:',t[3]\n",
    "    print '         Tweets:', t[4]\n",
    "    print '         at:', t[2]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very simple Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to familiarize ourselves with the dataset we started with a very simple approach for topic extraction. For achieving this we will use the Latent Dirichlet allocation algorithm. We first need to build the tf-idf matrix using our data and then then pass it as a parameter to the LDA method. We also need to specify the number of topics to be extracted from the dataset, α(parameter of the Dirichlet prior on the per-document topic distributions) and β(parameter of the Dirichlet prior on the per-topic word distribution). We will determine this values in the next milestone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|                id|            sentence|\n",
      "+------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|\n",
      "|345963923259924480|Can't stand peopl...|\n",
      "+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Take only the data having all 5 fields and in english\"\"\"\n",
    "en_data = data.filter(lambda x : x[:2]=='en')\n",
    "en_data = en_data.filter(selection_tweet)\n",
    "\n",
    "\"\"\"Encode UTF-8\"\"\"\n",
    "en_data = en_data.map(encode_tweet)\n",
    "\n",
    "\"\"\"Take only ID and CONTENT of a tweet\"\"\"\n",
    "tweets = en_data.map(lambda tweet : Row(id=tweet[1], sentence=tweet[4]))\n",
    "\n",
    "\"\"\"Create DF\"\"\"\n",
    "df_tweets = sqlContext.createDataFrame(tweets)\n",
    "\n",
    "df_tweets.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|                id|            sentence|                 raw|\n",
      "+------------------+--------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|[rt, silsilfani, ...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|[rt, whosthishoe,...|\n",
      "|345963923259924480|Can't stand peopl...|[can, t, stand, p...|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenization\"\"\"\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"raw\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(df_tweets)\n",
    "\n",
    "regexTokenized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|                id|            sentence|                 raw|            filtered|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|[rt, silsilfani, ...|[rt, silsilfani, ...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|[rt, whosthishoe,...|[rt, whosthishoe,...|\n",
      "|345963923259924480|Can't stand peopl...|[can, t, stand, p...|[t, stand, people...|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Remove Stop-words\"\"\"\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "removed_stopwords = remover.transform(regexTokenized)\n",
    "\n",
    "removed_stopwords.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lemmatization'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"apply lemmatization and remove punctuation\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Warning: Computation of TF-IDF and LDA take a lot of time (1h30 on the cluster). This is the reason why we have an extra python file(helloworld.py) to submit to the cluster for the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"TF-IDF\"\"\"\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "count_vectorizer_model = cv.fit(removed_stopwords)\n",
    "tf = count_vectorizer_model.transform(removed_stopwords)\n",
    "\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(tf)\n",
    "tfidf = idfModel.transform(tf)\n",
    "\n",
    "tfidf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Topics extraction with LDA\"\"\"\n",
    "\n",
    "nbTopics=100\n",
    "n_terms=15\n",
    "\n",
    "corpus = tfidf.select(F.col('id').cast(\"long\"), 'tfidf').rdd.map(lambda x: [x[0], x[1]])\n",
    "ldaModel = LDA.train(corpus, k=nbTopics)\n",
    "\n",
    "\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=n_terms)\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "\n",
    "\"\"\"Store result\"\"\"\n",
    "with open(\"topics.pickle\", \"wb\") as f:\n",
    "    pickle.dump(topics, f)\n",
    "with open(\"vocabulary.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vocabulary, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load result computed from cluster\"\"\"\n",
    "\n",
    "with open(\"topics.pickle\", \"rb\") as f:\n",
    "    topics = pickle.load(f)\n",
    "    \n",
    "with open(\"vocabulary.pickle\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for topic in range(len(topics)):\n",
    "    print(\"topic {} : \".format(topic))\n",
    "    words = topics[topic][0]\n",
    "    scores = topics[topic][1]\n",
    "    for word in range(len(words)):\n",
    "        print(vocabulary[words[word]], \"-\", scores[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the pipeline that we will follow for the LDA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: bigint, sentence: string]\n",
      "topic 0 : \n",
      "(u'regression', '-', 0.05145751141777857)\n",
      "(u'neat', '-', 0.05091900278065432)\n",
      "(u'logistic', '-', 0.05087718877476008)\n",
      "topic 1 : \n",
      "(u'coffee', '-', 0.0508638048864802)\n",
      "(u'case', '-', 0.05080822565440008)\n",
      "(u'want', '-', 0.05067566719691306)\n",
      "topic 2 : \n",
      "(u'hash', '-', 0.05181265926459157)\n",
      "(u'today', '-', 0.051378772846123556)\n",
      "(u'day', '-', 0.051329188339834325)\n"
     ]
    }
   ],
   "source": [
    "# random dataframe \n",
    "sentenceDataFrame = sqlContext.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\"),\n",
    "    (3, \"I want a coffee before going to bed \"),\n",
    "    (4, \"Today is a big day !!!\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "# tokenization\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "# remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "removed = remover.transform(regexTokenized)\n",
    "\n",
    "# create the tf-idf matrix\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "count_vectorizer_model = cv.fit(removed)\n",
    "tf = count_vectorizer_model.transform(removed)\n",
    "\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(tf)\n",
    "tfidf = idfModel.transform(tf)\n",
    "\n",
    "# initialize parameters\n",
    "nbTopics=3\n",
    "n_terms=3\n",
    "\n",
    "corpus = tfidf.select(F.col('id').cast(\"long\"), 'tfidf').rdd.map(lambda x: [x[0], x[1]])\n",
    "ldaModel = LDA.train(corpus, k=nbTopics)\n",
    "\n",
    "# extracting topics\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=n_terms)\n",
    "\n",
    "# extraction vocabulary\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "file = open(\"testfile.txt\",\"w\")\n",
    "for topic in range(len(topics)):\n",
    "    print(\"topic {} : \".format(topic))\n",
    "    file.write(\"topic {} : \\n\".format(topic)) \n",
    "    words = topics[topic][0]\n",
    "    scores = topics[topic][1]\n",
    "    for word in range(len(words)):\n",
    "        file.write(\"{} - {}\\n\".format(vocabulary[words[word]], scores[word]))\n",
    "        print(vocabulary[words[word]], \"-\", scores[word])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Milestone 3: the data story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Play around with the parameters of the LDA algorithm in order to find the optimal values for α and β. \"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"repeat the same technique for spanish and french\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"algorithm for detecting the top trends\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"trying to find patterns between trends\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"analyse the data per user \n",
    "    ex. which topics he tweets most about ? \n",
    "        does it change over time ? \"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"find top users that were the most mentioned by somebody else\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"visualization of the result(per language, per month ...)\"\"\"\n",
    "\n",
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
